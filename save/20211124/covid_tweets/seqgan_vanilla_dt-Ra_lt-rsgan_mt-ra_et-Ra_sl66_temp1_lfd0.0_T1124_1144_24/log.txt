====================================================================================================
> training arguments:
>>> if_test: 0
>>> run_model: seqgan
>>> k_label: 2
>>> dataset: covid_tweets
>>> model_type: vanilla
>>> loss_type: rsgan
>>> mu_type: ragan
>>> eval_type: Ra
>>> d_type: Ra
>>> if_real_data: 1
>>> cuda: 1
>>> device: 0
>>> devices: 0
>>> shuffle: 0
>>> gen_init: normal
>>> dis_init: uniform
>>> n_parent: 1
>>> eval_b_num: 8
>>> lambda_fq: 1.0
>>> lambda_fd: 0.0
>>> d_out_mean: True
>>> freeze_dis: False
>>> freeze_clas: False
>>> use_all_real_fake: False
>>> use_population: False
>>> samples_num: 10000
>>> vocab_size: 29329
>>> mle_epoch: 5
>>> clas_pre_epoch: 10
>>> adv_epoch: 3
>>> inter_epoch: 15
>>> batch_size: 64
>>> max_seq_len: 66
>>> start_letter: 1
>>> padding_idx: 0
>>> gen_lr: 0.01
>>> gen_adv_lr: 0.0001
>>> dis_lr: 0.0001
>>> clip_norm: 5.0
>>> pre_log_step: 10
>>> adv_log_step: 1
>>> train_data: dataset/covid_tweets.txt
>>> test_data: dataset/testdata/covid_tweets_test.txt
>>> temp_adpt: exp
>>> evo_temp_step: 1
>>> temperature: 1
>>> ora_pretrain: 1
>>> gen_pretrain: 0
>>> dis_pretrain: 0
>>> adv_g_step: 1
>>> rollout_num: 16
>>> gen_embed_dim: 32
>>> gen_hidden_dim: 32
>>> goal_size: 16
>>> step_size: 4
>>> mem_slots: 1
>>> num_heads: 2
>>> head_size: 256
>>> d_step: 5
>>> d_epoch: 3
>>> adv_d_step: 6
>>> adv_d_epoch: 2
>>> dis_embed_dim: 64
>>> dis_hidden_dim: 64
>>> num_rep: 64
>>> use_nll_oracle: 1
>>> use_nll_gen: 1
>>> use_nll_div: 1
>>> use_bleu: 1
>>> use_self_bleu: 1
>>> use_clas_acc: True
>>> use_ppl: 0
>>> log_file: log/log_1124_1144_24.txt
>>> save_root: save/20211124/covid_tweets/seqgan_vanilla_dt-Ra_lt-rsgan_mt-ra_et-Ra_sl66_temp1_lfd0.0_T1124_1144_24/
>>> signal_file: run_signal.txt
>>> tips: SeqGAN experiments
====================================================================================================
Starting Generator MLE Training...
[MLE-GEN] epoch 0 : pre_loss = 1.7889, BLEU-[2, 3, 4, 5] = [0.452, 0.171, 0.085, 0.056], NLL_gen = 1.5429, NLL_div = 1.842, Self-BLEU-[2, 3, 4] = [0.698, 0.347, 0.162], [PPL-F, PPL-R] = 0
[MLE-GEN] epoch 4 : pre_loss = 1.4096, BLEU-[2, 3, 4, 5] = [0.497, 0.214, 0.106, 0.066], NLL_gen = 1.3862, NLL_div = 1.4507, Self-BLEU-[2, 3, 4] = [0.759, 0.427, 0.206], [PPL-F, PPL-R] = 0
Starting Discriminator Training...
[MLE-DIS] d_step 0: d_loss = 0.1841, train_acc = 0.9317,
[MLE-DIS] d_step 1: d_loss = 0.0833, train_acc = 0.9740,
[MLE-DIS] d_step 2: d_loss = 0.0410, train_acc = 0.9891,
[MLE-DIS] d_step 3: d_loss = 0.0250, train_acc = 0.9937,
[MLE-DIS] d_step 4: d_loss = 0.0157, train_acc = 0.9962,
Starting Adversarial Training...
Initial generator: BLEU-[2, 3, 4, 5] = [0.492, 0.205, 0.102, 0.065], NLL_gen = 1.3862, NLL_div = 1.4393, Self-BLEU-[2, 3, 4] = [0.756, 0.414, 0.192], [PPL-F, PPL-R] = 0
-----
ADV EPOCH 0
-----
[ADV-GEN]: g_loss = 3196.3687, BLEU-[2, 3, 4, 5] = [0.502, 0.208, 0.095, 0.059], NLL_gen = 1.3895, NLL_div = 1.6712, Self-BLEU-[2, 3, 4] = [0.767, 0.446, 0.218], [PPL-F, PPL-R] = 0
[ADV-DIS] d_step 0: d_loss = 0.0259, train_acc = 0.9937,
[ADV-DIS] d_step 1: d_loss = 0.0214, train_acc = 0.9948,
[ADV-DIS] d_step 2: d_loss = 0.0200, train_acc = 0.9948,
[ADV-DIS] d_step 3: d_loss = 0.0148, train_acc = 0.9965,
[ADV-DIS] d_step 4: d_loss = 0.0140, train_acc = 0.9966,
[ADV-DIS] d_step 5: d_loss = 0.0126, train_acc = 0.9970,
-----
ADV EPOCH 1
-----
[ADV-GEN]: g_loss = 1834.4089, BLEU-[2, 3, 4, 5] = [0.542, 0.227, 0.103, 0.06], NLL_gen = 1.3943, NLL_div = 1.7687, Self-BLEU-[2, 3, 4] = [0.778, 0.473, 0.236], [PPL-F, PPL-R] = 0
[ADV-DIS] d_step 0: d_loss = 0.0084, train_acc = 0.9979,
[ADV-DIS] d_step 1: d_loss = 0.0073, train_acc = 0.9982,
[ADV-DIS] d_step 2: d_loss = 0.0094, train_acc = 0.9977,
[ADV-DIS] d_step 3: d_loss = 0.0070, train_acc = 0.9984,
[ADV-DIS] d_step 4: d_loss = 0.0062, train_acc = 0.9984,
[ADV-DIS] d_step 5: d_loss = 0.0065, train_acc = 0.9984,
-----
ADV EPOCH 2
-----
[ADV-GEN]: g_loss = 499.9774, BLEU-[2, 3, 4, 5] = [0.53, 0.219, 0.096, 0.056], NLL_gen = 1.3991, NLL_div = 1.8513, Self-BLEU-[2, 3, 4] = [0.786, 0.486, 0.245], [PPL-F, PPL-R] = 0
[ADV-DIS] d_step 0: d_loss = 0.0049, train_acc = 0.9988,
[ADV-DIS] d_step 1: d_loss = 0.0055, train_acc = 0.9985,
[ADV-DIS] d_step 2: d_loss = 0.0057, train_acc = 0.9985,
[ADV-DIS] d_step 3: d_loss = 0.0042, train_acc = 0.9989,
[ADV-DIS] d_step 4: d_loss = 0.0046, train_acc = 0.9988,
[ADV-DIS] d_step 5: d_loss = 0.0041, train_acc = 0.9990,
